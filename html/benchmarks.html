<!DOCTYPE html>
<!-- Copyright (c) 2019 Maxim Egorushkin. MIT License. See the full licence in file LICENSE. -->
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141287509-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-141287509-1');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="benchmarks.css">
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha256-pasqAKBDmFT4eHoN2ndd6lN370kFiGUFyTiUHWhU7k8=" crossorigin="anonymous"></script>
    <script src="https://code.highcharts.com/highcharts.js"></script>
    <script src="https://code.highcharts.com/modules/pattern-fill.js"></script>
    <script src="theme.js"></script>
    <script src="benchmarks.js"></script>
    <meta charset="utf-8">
    <title>Scalaibilty and Latency Benchmarks</title>
  </head>
  <body>
    <h1>Scalability Benchmark</h1>
    <p>N producer threads push a 4-byte integer into one queue, N consumer threads pop the integers from the queue. All producers posts 1,000,000 messages in total. Total time to send and receive all the messages is measured. The benchmark is run for from 1 producer and 1 consumer up to (total-number-of-cpus / 2) producers/consumers to measure the scalabilty of different queues. Different thread placements are tried to make sure the benchmark doesn't run into unexpected adverse scheduler or NUMA effects.</p>
    <div class="chart" id="scalability-7700k-5GHz"></div>
    <div class="chart" id="scalability-xeon-gold-6132"></div>
    <p>Note: Intel Xeon Gold 6132 has 2 sockets with 28 threads in total. Benchmarks use 1 NUMA node for up to 7 producers/consumers, 2 NUMA nodes are used for 8 producers/consumers or more. This is why throughput drops at 8 producers/consumers.</p>
    <h1>Latency Benchmark</h1>
    <p>One thread posts an integer to another thread and waits for the reply using two queues. The benchmarks measures the total time of 100,000 ping-pongs, best of 10 runs. Contention is minimal here to be able to achieve and measure the lowest latency. Reports the average round-trip time.</p>
    <div class="chart" id="latency-7700k-5GHz"></div>
    <div class="chart" id="latency-xeon-gold-6132"></div>
    <h2>Systems details</h2>
    <h3>Intel i7-7700K system</h3>
    <ul>
      <li>OS: Ubuntu-18.04.02 LTS
      <li>Compiler: gcc-8.3.0
      <li>atomic_queue version: commit e2c53cc932a8f426b2e4f0015c7fb90104ffb929
      <li>Boost version: 1.65.1
      <li>TBB version: 2019_U7, commit 4233fef583b4f8cbf9f781311717600feaaa0694
      <li>moodycamel concurrentqueue version: commit dea078cf5b6e742cd67a0d725e36f872feca4de4
      <li>moodycamel readerwriterqueue version: commit 2ae710de996a1d02bbc7696b2cdff2c6078e76f8
    </ul>
    <h3>Intel Xeon Gold 6132 system</h3>
    <ul>
      <li>OS: Red Hat Enterprise Linux Server release 6.10 (Santiago)
      <li>Compiler: gcc-5.4.0
      <li>atomic_queue version: commit e2c53cc932a8f426b2e4f0015c7fb90104ffb929
      <li>Boost version: 1.61.0
      <li>TBB version: 2019_U7, commit 4233fef583b4f8cbf9f781311717600feaaa0694
      <li>moodycamel concurrentqueue version: commit dea078cf5b6e742cd67a0d725e36f872feca4de4
      <li>moodycamel readerwriterqueue version: commit 2ae710de996a1d02bbc7696b2cdff2c6078e76f8
    </ul>
    <h3>Hyper-Threading</h3>
<p>When hyper-threading is enabled, single-producer-single-consumer benchmarks produce unrealistically great results if both the producer and consumer threads happen to get scheduled on the hyper-threads of the same core, due to L1d cache being shared between the cores. Such a thread placement, however, is undesirable in production scenarios because in CPU-bound workloads the two hyper-threads speedup is around 1.3x, whereas two non-hyper-threads speedup is 2x. For this reason the benchmarks pin threads to specific cores to avoid placing producers and consumers on the hyper-threads of the same core. Hyper-threads are still used in the benchmarks, but only when the number of producer and consumer threads is greater than the number of available non-hyper-threads.</p>
    <h3>Source Code</h3>
    <p><a href="https://github.com/max0x7ba/atomic_queue">github.com/max0x7ba/atomic_queue</a></p>
    <p class="copyright">Copyright (c) 2019 Maxim Egorushkin. MIT License. See the full licence in file LICENSE.</p>
  </body>
</html>
